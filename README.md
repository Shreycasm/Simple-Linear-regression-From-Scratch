# Simple Linear Regression with Ordinary Least Squares (OLS)



Linear regression is a fundamental concept in statistics and machine learning, serving as a powerful tool for modeling the relationship between two variables. In its simplest form, we have Simple Linear Regression, which explores the relationship between a dependent variable (target) and an independent variable (feature) through a linear equation.
The foundation of Simple Linear Regression lies in the Ordinary Least Squares (OLS) method, which aims to find the line that minimizes the sum of the squared differences between the observed and predicted values. The equation for Simple Linear Regression is given by:

y = mX + b

 - y is the dependent variable (target),
 - x is the independent variable (feature),
 - m is the slope or coefficient,
 - b is the y-intercept

![image](https://github.com/Shreycasm/Simple-Linear-regression-From-Scratch/assets/93276655/0033da7c-1fb1-46b9-ac68-dc2f0ad6a2c8)


These formulas help us find the optimal line that best fits the data by minimizing the sum of squared differences between the observed and predicted values.

Practical Implementation
In a practical scenario, we might use libraries like scikit-learn to perform Simple Linear Regression effortlessly. However, understanding the underlying OLS principles enhances our grasp of the algorithm's inner workings.

By implementing Simple Linear Regression from scratch, we gain insights into how coefficients are calculated, allowing us to appreciate the process beyond the abstraction provided by libraries.
